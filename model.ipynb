{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T08:15:51.602650Z",
     "start_time": "2025-05-06T08:15:49.919938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ],
   "id": "6a129a902fb72264",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T08:15:52.004168Z",
     "start_time": "2025-05-06T08:15:52.000895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "9239d5cb74589449",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:27:14.672995Z",
     "start_time": "2025-05-06T09:27:14.666538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot(seq):\n",
    "    conversion_dict={\n",
    "        'A':np.array([1.0,0.0,0.0,0.0]),\n",
    "        'C':np.array([0.0,1.0,0.0,0.0]),\n",
    "        'G':np.array([0.0,0.0,1.0,0.0]),\n",
    "        'T':np.array([0.0,0.0,0.0,1.0])\n",
    "    }\n",
    "    seqlen = len(seq)\n",
    "    enc_arr=conversion_dict[seq[0]]\n",
    "    for i in seq[1:]:\n",
    "        enc_arr=np.vstack((enc_arr,conversion_dict[i]))\n",
    "    enc_arr=enc_arr.T.reshape((1,4,50))\n",
    "    \n",
    "    return torch.tensor(enc_arr, dtype = torch.float32)\n",
    "        \n",
    "    "
   ],
   "id": "d4d4e9ce90124fa2",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T08:53:22.830212Z",
     "start_time": "2025-05-06T08:53:22.826899Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "133643e13cd54a37",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:27:18.963497Z",
     "start_time": "2025-05-06T09:27:18.957150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "testseq=\"AGACTAGCAGCATGCATCAGTCGCTAGCATCGATCGGATCGATGTCGTAG\"\n",
    "print(len(testseq))\n",
    "testtensor=one_hot(testseq)\n",
    "print(testtensor.shape)"
   ],
   "id": "ece7cbe76d3500f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "torch.Size([1, 4, 50])\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T09:11:03.857966Z",
     "start_time": "2025-05-06T09:10:55.618269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# load data\n",
    "dataset1=pd.read_csv('data/random_train_pc.csv')\n",
    "\n",
    "class fiveprimeutrs(Dataset):\n",
    "    def __init__(self, seqs, mrl):\n",
    "        self.seqs = seqs\n",
    "        self.mrl = mrl\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seqs[idx]\n",
    "        mrl = self.mrl[idx]\n",
    "        "
   ],
   "id": "814e906b43ba192e",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:20:42.836650Z",
     "start_time": "2025-05-06T13:20:42.829146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 120, kernel_size = 8, padding='same')\n",
    "        self.conv2 = nn.Conv1d(120, 120, kernel_size = 8, padding='same')\n",
    "        self.conv3 = nn.Conv1d(120, 120, kernel_size = 8, padding='same')\n",
    "        #self.flat1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(120, 40)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        print(x.shape)\n",
    "        #x = self.flat1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "bc820dd33b03927d",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:20:49.269127Z",
     "start_time": "2025-05-06T13:20:49.260098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net=Model()\n",
    "net.to(device)"
   ],
   "id": "8d70f9b8d79e61ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv1d(4, 120, kernel_size=(8,), stride=(1,), padding=same)\n",
       "  (conv2): Conv1d(120, 120, kernel_size=(8,), stride=(1,), padding=same)\n",
       "  (conv3): Conv1d(120, 120, kernel_size=(8,), stride=(1,), padding=same)\n",
       "  (fc1): Linear(in_features=120, out_features=40, bias=True)\n",
       "  (drop1): Dropout(p=0.2, inplace=False)\n",
       "  (out): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T13:20:53.718314Z",
     "start_time": "2025-05-06T13:20:53.658051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "testtensor=one_hot(testseq)\n",
    "train_dat=torch.utils.data.TensorDataset(testtensor.to(device))\n",
    "train_loader=torch.utils.data.DataLoader(train_dat, batch_size=1, shuffle=False)\n",
    "test_loader=torch.utils.data.DataLoader(testtensor, batch_size=1, shuffle=False)\n",
    "print(testtensor.shape)\n",
    "net(testtensor)\n",
    "# zum error: um kernel size 8,4 zu verwnden muss es eine 2d convolutional layer sein und nicht 1d, ich muss nur den input reshapen\n"
   ],
   "id": "a35cdbad3b9af27b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50])\n",
      "torch.Size([1, 120, 50])\n",
      "torch.Size([1, 120, 50])\n",
      "torch.Size([1, 120, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (120x50 and 120x40)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[128], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m test_loader\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(testtensor, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(testtensor\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtesttensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# zum error: um kernel size 8,4 zu verwnden muss es eine 2d convolutional layer sein und nicht 1d, ich muss nur den input reshapen\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[126], line 22\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#x = self.flat1(x)\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop1(x)\n\u001B[1;32m     24\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout(x)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (120x50 and 120x40)"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d2ac686884690e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
